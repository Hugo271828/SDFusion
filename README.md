# SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation
[[`arXiv`](https://arxiv.org/abs/2203.09516)]
[[`Project Page`](https://yccyenchicheng.github.io/SDFusion/)]
[[`BibTex`](#citation)]

![1-teaser-v3-out-1](https://user-images.githubusercontent.com/27779063/206553254-582cb70f-4174-45db-a254-8c00dac4662e.png)
SDFusion is a diffusion-based 3D shape generator. It enables various applications. (left) SDFusion can generate 3D shapes conditioned on different input modalities, including partial shapes, images, and text. SDFusion can even jointly handle multiple conditioning modalities while controlling the strength for each of them. (right) We showcase an application where we leverage pretrained 2D models to texture 3D shapes generated by SDFusion.

We also use a 3D-printer to print out the generated shapes of SDFusion.

https://user-images.githubusercontent.com/27779063/206553305-e01009f7-3131-4a6b-bda7-572699d97338.mp4


# Code

Coming soon!
<!-- Code for "SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation" (coming soon!). -->

# <a name="citation"></a>Citing SDFusion

<!-- If you find this code helpful, please consider citing: -->

```BibTeX
@inproceedings{cheng2022,
  title={{SDFusion}: Multimodal 3D Shape Completion, Reconstruction, and Generation},
  author={Cheng, Yen-Chi and Lee, Hsin-Ying and Tulyakov, Sergey and Schwing, Alex and Gui, Liangyan},
  journal={arXiv preprint arXiv:2212.00792},
  year={2022}
}
```

# Acknowledgement
Work supported in part by NSF under Grants  2008387, 2045586, 2106825, MRI 1725729, and NIFA award 2020-67021-32799. Thanks to NVIDIA for providing a GPU for debugging.
